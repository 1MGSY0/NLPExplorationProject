{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f050e572",
   "metadata": {},
   "source": [
    "<h3>Summarisation with Textrank</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed32d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanwe\\Desktop\\School Stuff\\GitHub\\NLPExplorationProject\\textrank\\Lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566bbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/tanwe/Desktop/School Stuff/GitHub/NLPExplorationProject/data/Dataset1/test_summary.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b6938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n"
     ]
    }
   ],
   "source": [
    "# Function for summarisation, uses TextRank \n",
    "def summarise_reviews(df, col, lim_phrase, lim_sent): \n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    nlp.add_pipe(\"textrank\")   \n",
    "\n",
    "    summarised = []\n",
    "    sentences = 0\n",
    "    index = 0\n",
    "    for review in df[col]:\n",
    "        doc = nlp(review)\n",
    "\n",
    "        for sent in doc._.textrank.summary(limit_phrases = lim_phrase, limit_sentences = lim_sent):\n",
    "            if sentences == 0:\n",
    "                summarised += [sent.text]\n",
    "            else:\n",
    "                summarised[index] += sent.text\n",
    "            sentences += 1\n",
    "\n",
    "            if sentences == lim_sent:\n",
    "                index += 1\n",
    "                sentences = 0\n",
    "    return summarised\n",
    "            \n",
    "summarised_lst = summarise_reviews(df, \"review/text\", 5, 5)  \n",
    "print(len(summarised_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a6fa6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moreover, the authors seems to be quite qualified.I was prepared to be annoyed and dislike this book, I was fully prepared to feel let down by its casual reference to various books and eras - but I have come away from it inspired by it.No way - I just can't get into the ultra voilence of that kind of book - and Tristram Shandy - no way - I've tried that it was virtually unreadable.There will always be arguments about other books which should be included and ones which should be left out - and I can think of several in both cases, however all technical arguments aside, if you are going to pick 1001 books through the entire history of printing to put in a list, I think this has been an excellent job.The editor, Peter Boxall, did an excellent job on keeping each review to a page, and ensuring that each of them made excellent reading on their own.\n"
     ]
    }
   ],
   "source": [
    "print(summarised_lst[881])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ca9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was prepared to be annoyed and dislike this book, I was fully prepared to feel let down by its casual reference to various books and eras - but I have come away from it inspired by it. I think it is a perfect resource for what it seems to set out to do - list 1001 books that you should read before you die.Well now, here is the issue. There is no way I am going to read some of them - American Psycho? No way - I just can't get into the ultra voilence of that kind of book - and Tristram Shandy - no way - I've tried that it was virtually unreadable. What the book does extremely well, however, is that it reviews the book succinctly and puts the book in context for not its own time and in history.The editor, Peter Boxall, did an excellent job on keeping each review to a page, and ensuring that each of them made excellent reading on their own. It is a bit like reading a good Haiku after you have read the Lady of Shallot, both are excellent, but the Haiku leaves you savouring for more. Each review I read left me wanting to read more.There will always be arguments about other books which should be included and ones which should be left out - and I can think of several in both cases, however all technical arguments aside, if you are going to pick 1001 books through the entire history of printing to put in a list, I think this has been an excellent job.Certainly from my especial interest in Early nineteenth century literature (Jane Austen, Edgeworth, Shelley and co) I felt the list was comprehensive, the selection very reflective of the times, and the reviews were exceptionally well done.Overall I think this is an inspiring book, well done and extremely useful.\n"
     ]
    }
   ],
   "source": [
    "print(df['review/text'].iloc[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb1ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "ref_summary = []\n",
    "for summary in df['review/summary']:\n",
    "    ref_summary.append(summary)\n",
    "print(len(ref_summary))\n",
    "print(type(ref_summary[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b3b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanwe\\Desktop\\School Stuff\\GitHub\\NLPExplorationProject\\textrank\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 6.14kB [00:00, ?B/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (883) and references (1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\n\u001b[32m      2\u001b[39m rouge = evaluate.load(\u001b[33m'\u001b[39m\u001b[33mrouge\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mrouge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummarised_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanwe\\Desktop\\School Stuff\\GitHub\\NLPExplorationProject\\textrank\\Lib\\site-packages\\evaluate\\module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanwe\\Desktop\\School Stuff\\GitHub\\NLPExplorationProject\\textrank\\Lib\\site-packages\\evaluate\\module.py:546\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    540\u001b[39m     error_msg = (\n\u001b[32m    541\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and/or references don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.selected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    543\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    544\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Mismatch in the number of predictions (883) and references (1000)"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "results = rouge.compute(predictions=summarised_lst, references=ref_summary)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrank (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
