{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64c99bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers \n",
    "import datasets \n",
    "import evaluate \n",
    "import rouge_score\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7385c436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review/summary', 'review/text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"train\": \"../data/Dataset1/train_summary.csv\", \"validation\": \"../data/Dataset1/val_summary.csv\", \"test\": \"../data/Dataset1/test_summary.csv\"}\n",
    "\n",
    "# reduce number of training samples to help with training\n",
    "train_ds = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\", split=\"train[:25%]\")\n",
    "val_ds = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\", split=\"validation[:25%]\")\n",
    "test_ds = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\", split=\"test\")\n",
    "\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d33d1830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns to align with hf dataset format\n",
    "\n",
    "train_ds = train_ds.rename_column(\"review/summary\", \"summary\")\n",
    "val_ds = val_ds.rename_column(\"review/summary\", \"summary\")\n",
    "test_ds = test_ds.rename_column(\"review/summary\", \"summary\")\n",
    "\n",
    "train_ds = train_ds.rename_column(\"review/text\", \"text\")\n",
    "val_ds = val_ds.rename_column(\"review/text\", \"text\")\n",
    "test_ds = test_ds.rename_column(\"review/text\", \"text\")\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5836c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for preprocessing\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e76107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5890.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \" # instructions included in prompt for the model\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # tokenize labels\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # replace padding token id with -100 for T5 loss\n",
    "    labels = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# apply the preprocessing function, batch for faster mapping\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# pad the text in each review to the longest length in a batch\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d212a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4de4f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sorui\\Desktop\\NLPExplorationProject\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sorui\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e692983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 12:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.114164</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>19.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.038428</td>\n",
       "      <td>0.141400</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>19.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.979244</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>19.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.929502</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>19.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.885674</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.129300</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>19.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.851595</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>19.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.826695</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>19.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.811897</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>19.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.801108</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>19.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.796399</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>19.096000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=5.022878646850586, metrics={'train_runtime': 761.1054, 'train_samples_per_second': 13.139, 'train_steps_per_second': 0.026, 'total_flos': 2032324211638272.0, 'train_loss': 5.022878646850586, 'epoch': 10.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to speed up training, freeze the encoder layer\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# freeze all decoder layers except last\n",
    "num_decoder_layers = len(model.decoder.block)\n",
    "for i, block in enumerate(model.decoder.block):\n",
    "    if i != num_decoder_layers - 1:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# retain language model head for training\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_summarizer\",\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,                  # higher LR since fewer params update\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=32,      # simulate batch size 32\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,         # huge memory reduction\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9a38d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.019801980198019802), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0165016501650165), 'rougeLsum': np.float64(0.0165016501650165)}\n"
     ]
    }
   ],
   "source": [
    "predictions = [test_ds[\"summary\"][0]]\n",
    "references = [test_ds[\"text\"][0]]\n",
    "results = rouge.compute(predictions = predictions, references = references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d481854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc828cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.09580201404263411, 'rouge2': 0.021806565429959714, 'rougeL': 0.08418221112029639, 'rougeLsum': 0.08417014998309315}\n"
     ]
    }
   ],
   "source": [
    "# conduct inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"\\Users\\sorui\\Desktop\\NLPExplorationProject\\Application-2\\t5_summarizer\\checkpoint-20\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(r\"\\Users\\sorui\\Desktop\\NLPExplorationProject\\Application-2\\t5_summarizer\\checkpoint-20\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in tqdm(range(len(test_ds[\"summary\"])), desc = \"Generating summaries\"):\n",
    "    references.append(test_ds[\"summary\"][i])\n",
    "    inputs = tokenizer(test_ds[\"text\"][i], return_tensors=\"pt\", truncation=True).input_ids\n",
    "    outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "    predictions.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    \n",
    "results = rouge.compute(predictions = predictions, references = references, use_aggregator=True)\n",
    "results = {k: float(v) for k, v in results.items()}\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
