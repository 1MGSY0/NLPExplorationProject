{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6919db",
   "metadata": {},
   "source": [
    "- App B: News categorisation\n",
    "  - Traditional: TF-IDF + K-means\n",
    "  - Neural: BERT-sentence + K-means\n",
    "  - Metrics: Silhouette, Davies Bouldin, Calinski Harabasz scores + observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf107c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesse Tham\\Desktop\\School Materials\\Year 4 Sem 1\\EE6405 Final Project\\NLPExplorationProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Jesse Tham\\Desktop\\School Materials\\Year 4 Sem 1\\EE6405 Final Project\\NLPExplorationProject\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "NUM_CLUSTERS = 8\n",
    "MAX_DOCS_FOR_METRICS = 3000  \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # remove URLs\n",
    "    text = re.sub(r\"\\(cnn\\)\\s*--\", \"\", text)        # remove minimal CNN header\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)            # remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \" \", text)                # remove numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()        # normalize whitespace\n",
    "\n",
    "    return text\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])  \n",
    "def lemmatise(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if token.lemma_.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115caa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'article', 'highlights'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "trainAll_df = pd.read_csv(\"../../data/Dataset2/train.csv\")\n",
    "\n",
    "print(trainAll_df.columns)\n",
    "trainAll_df.head()\n",
    "\n",
    "train_df = trainAll_df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7923bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text... this may take a few minutes.\n",
      "                                             article  \\\n",
      "0  By . Mia De Graaf . Britons flocked to beaches...   \n",
      "1  A couple who weighed a combined 32st were sham...   \n",
      "2  Video footage shows the heart stopping moment ...   \n",
      "3  Istanbul, Turkey (CNN) -- About 250 people rac...   \n",
      "4  By . Daily Mail Reporter . PUBLISHED: . 12:53 ...   \n",
      "\n",
      "                                       clean_article  \n",
      "0  by mia de graaf britons flocked to beaches acr...  \n",
      "1  a couple who weighed a combined st were shamed...  \n",
      "2  video footage shows the heart stopping moment ...  \n",
      "3  istanbul turkey about people raced across the ...  \n",
      "4  by daily mail reporter published est january u...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing text... this may take a few minutes.\")\n",
    "\n",
    "train_df['clean_article'] = train_df['article'].apply(preprocess)\n",
    "\n",
    "print(train_df[['article', 'clean_article']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd47c74",
   "metadata": {},
   "source": [
    "Traditional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01e4425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesse Tham\\Desktop\\School Materials\\Year 4 Sem 1\\EE6405 Final Project\\NLPExplorationProject\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jesse Tham\\Desktop\\School Materials\\Year 4 Sem 1\\EE6405 Final Project\\NLPExplorationProject\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['I', 'far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature shape: (10000, 10000)\n",
      "Fitting KMeans on TF-IDF features...\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF w stop-word removal & lemmantization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lemmatise,\n",
    "    max_features=10000,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(train_df['clean_article'])\n",
    "\n",
    "print(\"TF-IDF feature shape:\", tfidf_features.shape)\n",
    "\n",
    "# K-means\n",
    "kmeans_tfidf = KMeans(\n",
    "    n_clusters=NUM_CLUSTERS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "print(\"Fitting KMeans on TF-IDF features...\")\n",
    "kmeans_tfidf.fit(tfidf_features)\n",
    "\n",
    "tfidf_cluster_labels = kmeans_tfidf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae6fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF Cluster 0 ===\n",
      "s, say, u, al, military, attack, government, security, syria, iraq, official, country, force, isis, president\n",
      "\n",
      "=== TF-IDF Cluster 1 ===\n",
      "s, obama, say, president, party, mr, election, tax, government, vote, I, republican, minister, house, cameron\n",
      "\n",
      "=== TF-IDF Cluster 2 ===\n",
      "say, s, court, child, I, year, school, family, mr, sentence, tell, charge, police, prison, case\n",
      "\n",
      "=== TF-IDF Cluster 3 ===\n",
      "s, league, player, club, game, cup, season, goal, team, I, win, play, score, manchester, match\n",
      "\n",
      "=== TF-IDF Cluster 4 ===\n",
      "police, say, officer, s, arrest, man, car, I, year, report, shoot, suspect, tell, old, charge\n",
      "\n",
      "=== TF-IDF Cluster 5 ===\n",
      "patient, say, health, hospital, s, disease, doctor, study, cancer, dr, cent, people, drug, woman, treatment\n",
      "\n",
      "=== TF-IDF Cluster 6 ===\n",
      "s, say, I, year, new, make, use, people, t, world, million, time, water, like, company\n",
      "\n",
      "=== TF-IDF Cluster 7 ===\n",
      "I, s, say, t, m, just, year, think, like, know, time, want, feel, make, tell\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_words_per_cluster(kmeans_model, feature_names, top_n=15):\n",
    "    cluster_keywords = []\n",
    "    for cluster_id in range(kmeans_model.n_clusters):\n",
    "        center = kmeans_model.cluster_centers_[cluster_id]\n",
    "        top_idx = np.argsort(center)[-top_n:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_idx]\n",
    "        cluster_keywords.append(top_words)\n",
    "    return cluster_keywords\n",
    "\n",
    "cluster_keywords_tfidf = get_top_words_per_cluster(kmeans_tfidf, feature_names, top_n=15)\n",
    "\n",
    "for i, words in enumerate(cluster_keywords_tfidf):\n",
    "    print(f\"\\n=== TF-IDF Cluster {i} ===\")\n",
    "    print(\", \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8adbc2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + KMeans metrics:\n",
      "  Silhouette score       : 0.005059910155776732\n",
      "  Davies-Bouldin index   : 9.372211147001494\n",
      "  Calinski-Harabasz index: 11.56267222842952\n"
     ]
    }
   ],
   "source": [
    "# Subsample for metrics\n",
    "num_samples_for_metrics = min(MAX_DOCS_FOR_METRICS, tfidf_features.shape[0])\n",
    "eval_indices_tfidf = np.random.choice(\n",
    "    tfidf_features.shape[0],\n",
    "    size=num_samples_for_metrics,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "tfidf_eval = tfidf_features[eval_indices_tfidf].toarray()\n",
    "tfidf_labels_eval = tfidf_cluster_labels[eval_indices_tfidf]\n",
    "\n",
    "silhouette_tfidf = silhouette_score(tfidf_eval, tfidf_labels_eval)\n",
    "davies_bouldin_tfidf = davies_bouldin_score(tfidf_eval, tfidf_labels_eval)\n",
    "calinski_harabasz_tfidf = calinski_harabasz_score(tfidf_eval, tfidf_labels_eval)\n",
    "\n",
    "print(\"TF-IDF + KMeans metrics:\")\n",
    "print(\"  Silhouette score       :\", silhouette_tfidf)\n",
    "print(\"  Davies-Bouldin index   :\", davies_bouldin_tfidf)\n",
    "print(\"  Calinski-Harabasz index:\", calinski_harabasz_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b9104",
   "metadata": {},
   "source": [
    "Neural Model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af87952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesse Tham\\Desktop\\School Materials\\Year 4 Sem 1\\EE6405 Final Project\\NLPExplorationProject\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding articles with Sentence-BERT model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [11:50<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-BERT embedding shape: (10000, 384)\n",
      "Fitting KMeans on Sentence-BERT embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Sentence-BERT\n",
    "sbert_model_name = \"all-MiniLM-L6-v2\" \n",
    "sbert_model = SentenceTransformer(sbert_model_name)\n",
    "\n",
    "print(f\"Encoding articles with Sentence-BERT model: {sbert_model_name}...\")\n",
    "sbert_embeddings = sbert_model.encode(\n",
    "    train_df['clean_article'],\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True, \n",
    ")\n",
    "\n",
    "print(\"Sentence-BERT embedding shape:\", sbert_embeddings.shape)\n",
    "\n",
    "# K-means\n",
    "kmeans_sbert = KMeans(\n",
    "    n_clusters=NUM_CLUSTERS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "print(\"Fitting KMeans on Sentence-BERT embeddings...\")\n",
    "kmeans_sbert.fit(sbert_embeddings)\n",
    "\n",
    "sbert_cluster_labels = kmeans_sbert.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eab58cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SBERT Cluster 0 ===\n",
      "said, people, police, mr, hospital, family, year, told, just, car, new, home, died, water, old\n",
      "\n",
      "=== SBERT Cluster 1 ===\n",
      "said, mr, people, year, cent, london, new, police, million, government, uk, bbc, cameron, years, tax\n",
      "\n",
      "=== SBERT Cluster 2 ===\n",
      "said, al, government, people, police, military, president, syria, security, iraq, cnn, north, country, pakistan, state\n",
      "\n",
      "=== SBERT Cluster 3 ===\n",
      "said, women, year, just, like, new, time, family, ms, says, people, years, miss, old, mother\n",
      "\n",
      "=== SBERT Cluster 4 ===\n",
      "said, obama, president, new, house, state, people, cnn, year, romney, republican, government, white, time, told\n",
      "\n",
      "=== SBERT Cluster 5 ===\n",
      "united, league, club, world, cup, season, team, england, year, said, chelsea, city, game, players, manchester\n",
      "\n",
      "=== SBERT Cluster 6 ===\n",
      "said, police, court, year, mr, told, old, home, man, school, years, family, death, mother, children\n",
      "\n",
      "=== SBERT Cluster 7 ===\n",
      "said, new, people, world, year, like, years, million, just, time, says, mr, company, used, cent\n"
     ]
    }
   ],
   "source": [
    "def get_sbert_cluster_keywords(df, labels, text_col=\"clean_article\", top_n=15):\n",
    "    cluster_keywords = []\n",
    "    for c in range(NUM_CLUSTERS):\n",
    "        cluster_texts = df[text_col][labels == c]\n",
    "        if len(cluster_texts) == 0:\n",
    "            cluster_keywords.append([])\n",
    "            continue\n",
    "        \n",
    "        vec = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "        X = vec.fit_transform(cluster_texts)\n",
    "        feats = vec.get_feature_names_out()\n",
    "\n",
    "        mean_scores = X.mean(axis=0).A1\n",
    "        top_idx = np.argsort(mean_scores)[-top_n:][::-1]\n",
    "        top_words = [feats[i] for i in top_idx]\n",
    "        cluster_keywords.append(top_words)\n",
    "    return cluster_keywords\n",
    "\n",
    "cluster_keywords_sbert = get_sbert_cluster_keywords(\n",
    "    train_df,\n",
    "    sbert_cluster_labels,\n",
    "    text_col=\"clean_article\" \n",
    ")\n",
    "\n",
    "for i, words in enumerate(cluster_keywords_sbert):\n",
    "    print(f\"\\n=== SBERT Cluster {i} ===\")\n",
    "    print(\", \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf3c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-BERT + KMeans metrics:\n",
      "  Silhouette score       : 0.032252975\n",
      "  Davies-Bouldin index   : 4.532999471801748\n",
      "  Calinski-Harabasz index: 57.72461755129285\n"
     ]
    }
   ],
   "source": [
    "num_samples_for_metrics_sbert = min(MAX_DOCS_FOR_METRICS, sbert_embeddings.shape[0])\n",
    "eval_indices_sbert = np.random.choice(\n",
    "    sbert_embeddings.shape[0],\n",
    "    size=num_samples_for_metrics_sbert,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "sbert_eval = sbert_embeddings[eval_indices_sbert]\n",
    "sbert_labels_eval = sbert_cluster_labels[eval_indices_sbert]\n",
    "\n",
    "silhouette_sbert = silhouette_score(sbert_eval, sbert_labels_eval)\n",
    "davies_bouldin_sbert = davies_bouldin_score(sbert_eval, sbert_labels_eval)\n",
    "calinski_harabasz_sbert = calinski_harabasz_score(sbert_eval, sbert_labels_eval)\n",
    "\n",
    "print(\"Sentence-BERT + KMeans metrics:\")\n",
    "print(\"  Silhouette score       :\", silhouette_sbert)\n",
    "print(\"  Davies-Bouldin index   :\", davies_bouldin_sbert)\n",
    "print(\"  Calinski-Harabasz index:\", calinski_harabasz_sbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c776c1",
   "metadata": {},
   "source": [
    "Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2906b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Silhouette</th>\n",
       "      <th>Davies_Bouldin</th>\n",
       "      <th>Calinski_Harabasz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF + KMeans</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>9.372211</td>\n",
       "      <td>11.562672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence-BERT + KMeans</td>\n",
       "      <td>0.032253</td>\n",
       "      <td>4.532999</td>\n",
       "      <td>57.724618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Silhouette  Davies_Bouldin  Calinski_Harabasz\n",
       "0         TF-IDF + KMeans    0.005060        9.372211          11.562672\n",
       "1  Sentence-BERT + KMeans    0.032253        4.532999          57.724618"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"TF-IDF + KMeans\", \"Sentence-BERT + KMeans\"],\n",
    "        \"Silhouette\": [silhouette_tfidf, silhouette_sbert],\n",
    "        \"Davies_Bouldin\": [davies_bouldin_tfidf, davies_bouldin_sbert],\n",
    "        \"Calinski_Harabasz\": [calinski_harabasz_tfidf, calinski_harabasz_sbert],\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
